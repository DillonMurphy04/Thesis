@Article{tallam2023,
AUTHOR = {Tallam, Krti and Nguyen, Nam and Ventura, Jonathan and Fricker, Andrew and Calhoun, Sadie and O’Leary, Jennifer and Fitzgibbons, Mauriça and Robbins, Ian and Walter, Ryan K.},
TITLE = {Application of Deep Learning for Classification of Intertidal Eelgrass from Drone-Acquired Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {15},
YEAR = {2023},
NUMBER = {9},
ARTICLE-NUMBER = {2321},
URL = {https://www.mdpi.com/2072-4292/15/9/2321},
ISSN = {2072-4292},
ABSTRACT = {Shallow estuarine habitats are globally undergoing rapid changes due to climate change and anthropogenic influences, resulting in spatiotemporal shifts in distribution and habitat extent. Yet, scientists and managers do not always have rapidly available data to track habitat changes in real-time. In this study, we apply a novel and a state-of-the-art image segmentation machine learning technique (DeepLab) to two years of high-resolution drone-based imagery of a marine flowering plant species (eelgrass, a temperate seagrass). We apply the model to eelgrass (Zostera marina) meadows in the Morro Bay estuary, California, an estuary that has undergone large eelgrass declines and the subsequent recovery of seagrass meadows in the last decade. The model accurately classified eelgrass across a range of conditions and sizes from meadow-scale to small-scale patches that are less than a meter in size. The model recall, precision, and F1 scores were 0.954, 0.723, and 0.809, respectively, when using human-annotated training data and random assessment points. All our accuracy values were comparable to or demonstrated greater accuracy than other models for similar seagrass systems. This study demonstrates the potential for advanced image segmentation machine learning methods to accurately support the active monitoring and analysis of seagrass dynamics from drone-based images, a framework likely applicable to similar marine ecosystems globally, and one that can provide quantitative and accurate data for long-term management strategies that seek to protect these vital ecosystems.},
DOI = {10.3390/rs15092321}
}

@misc{angelopoulos2021,
      title={A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification}, 
      author={Anastasios N. Angelopoulos and Stephen Bates},
      year={2022},
      eprint={2107.07511},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.07511}, 
}

@article{dewolf2025,
	title = {Conditional validity of heteroskedastic conformal regression},
	volume = {14},
	issn = {2049-8772},
	url = {https://doi.org/10.1093/imaiai/iaaf013},
	doi = {10.1093/imaiai/iaaf013},
	abstract = {Conformal prediction, and split conformal prediction as a specific implementation, offers a distribution-free approach to estimating prediction intervals with statistical guarantees. Recent work has shown that split conformal prediction can produce state-of-the-art prediction intervals when focusing on marginal coverage, i.e. on a calibration dataset, the method produces on average prediction intervals that contain the ground truth with a predefined coverage level. However, such intervals are often not adaptive, which can be problematic for regression problems with heteroskedastic noise. This paper tries to shed new light on how prediction intervals can be constructed, using methods such as normalized and Mondrian conformal prediction, in such a way that they adapt to the heteroskedasticity of the underlying process. Theoretical and experimental results are presented, in which these methods are compared in a systematic way. In particular, it is shown how the conditional validity of a chosen conformal predictor can be related to (implicit) assumptions about the data-generating distribution.},
	number = {2},
	journal = {Information and Inference: A Journal of the IMA},
	author = {Dewolf, Nicolas and De Baets, Bernard and Waegeman, Willem},
	month = may,
	year = {2025},
	note = {\_eprint: https://academic.oup.com/imaiai/article-pdf/14/2/iaaf013/63165177/iaaf013.pdf},
	pages = {iaaf013},
}

@inproceedings{lakshminarayanan2017,
author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
title = {Simple and scalable predictive uncertainty estimation using deep ensembles},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6405–6416},
numpages = {12},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@inproceedings{kendall2017,
 author = {Kendall, Alex and Gal, Yarin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf},
 volume = {30},
 year = {2017}
}


@InProceedings{guo2017,
  title = 	 {On Calibration of Modern Neural Networks},
  author =       {Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1321--1330},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/guo17a/guo17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/guo17a.html},
  abstract = 	 {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.}
}

@inbook{ovadia2019,
author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua V. and Lakshminarayanan, Balaji and Snoek, Jasper},
title = {Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive uncertainty. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1254},
numpages = {12}
}

@article{lei2018,
author = {Jing Lei and Max G’Sell and Alessandro Rinaldo and Ryan J. Tibshirani and Larry Wasserman},
title = {Distribution-Free Predictive Inference for Regression},
journal = {Journal of the American Statistical Association},
volume = {113},
number = {523},
pages = {1094--1111},
year = {2018},
publisher = {ASA Website},
doi = {10.1080/01621459.2017.1307116},
URL = { 
        https://doi.org/10.1080/01621459.2017.1307116
},
eprint = { 
        https://doi.org/10.1080/01621459.2017.1307116
}
}

@article{
chernozhukov2021,
author = {Victor Chernozhukov  and Kaspar Wüthrich  and Yinchu Zhu },
title = {Distributional conformal prediction},
journal = {Proceedings of the National Academy of Sciences},
volume = {118},
number = {48},
pages = {e2107794118},
year = {2021},
doi = {10.1073/pnas.2107794118},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2107794118},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2107794118},
abstract = {Prediction problems are important in many contexts. Examples include cross-sectional prediction, time series forecasting, counterfactual prediction and synthetic controls, and individual treatment effect prediction. We develop a prediction method that works in conjunction with many powerful classical methods (e.g., conventional quantile regression) as well as modern high-dimensional methods for estimating conditional distributions (e.g., quantile neural networks). Unlike many existing prediction approaches, our method is valid conditional on the observed predictors and efficient under some conditions. Importantly, our method is also robust; it exhibits unconditional coverage guarantees under model misspecification, under overfitting, and with time series data. We propose a robust method for constructing conditionally valid prediction intervals based on models for conditional distributions such as quantile and distribution regression. Our approach can be applied to important prediction problems, including cross-sectional prediction, k–step-ahead forecasts, synthetic controls and counterfactual prediction, and individual treatment effects prediction. Our method exploits the probability integral transform and relies on permuting estimated ranks. Unlike regression residuals, ranks are independent of the predictors, allowing us to construct conditionally valid prediction intervals under heteroskedasticity. We establish approximate conditional validity under consistent estimation and provide approximate unconditional validity under model misspecification, under overfitting, and with time series data. We also propose a simple “shape” adjustment of our baseline method that yields optimal prediction intervals.}}


