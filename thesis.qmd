---
format:
  pdf:
    documentclass: report
    papersize: letter
    fontsize: 12pt
    mainfont: Times New Roman
    geometry:
      - left=1.5in
      - right=1in
      - top=1in
      - bottom=1in
    include-before-body:
      - frontmatter/information.tex
      - frontmatter/title-page.tex
      - frontmatter/copyright-page.tex
      - frontmatter/committee-page.tex
      - frontmatter/abstract.tex
      - frontmatter/acknowledgments.tex
      - frontmatter/table-of-contents.tex
    number-sections: true
    citation-style: apa
    bibliography: bibliography/example-bibliography.bib
    csl: bibliography/apa-6th-edition.csl
    include-in-header: frontmatter/formating.tex
---

# Introduction

Eelgrass (*Zostera marina*) is a seagrass species of temperate waters that provides an anchor to coastal ecosystems by providing sediment stabilization, carbon sequestration, eliminating contamination, and providing habitats for protected species. Warming oceans, storm regimes, and local disturbance have quickly reshaped the intertidal morphology and habitat extent. As these threats increase, there is a need to monitor changes to determine appropriate environmental management. In recent years, deep networks have made this intertidal mapping practical in complex imagery, achieving strong pixel-wise accuracy in areas such as Morro Bay, California [@tallam2023]. However, the reliability of general utilization of these classification models not only relies on high-accuracy segmentation, but also on rigorous uncertainty quantification under temporal and spatial distribution shift. Imagery collected over different years, at different tides or seasons, can erode the model's calibration, causing it to be over-confident. This study builds off these models to create a post hoc uncertainty framework.

Uncertainty quantification is a requirement for flagging ambiguous areas. To answer this, split conformal prediction provides an appealing framework. It offers a wrapping of any probabilistic predictor to produce prediction sets with finite-sample marginal coverage under exchangeability, simply by using a calibration set and a quantile nonconformity score [@angelopoulos2021]. In classification, a natural choice of noncormity score is $s=1-p_y$, yielding a prediction set of all labels whose scores do not exceed a global threshold learned on calibration. With stable data, this can be simple and effective; however, in reality, the distribution of scores can differ by difficulty. A threshold calibrated over one year can underperform when a distributional shift occurs, such as temporal drift.

To remedy this, it is necessary to make the conformal prediction adaptive to drift or local difficulty. First, work in adaptive or locally weighted CP in regression has shown that normalizing scores by a measure of local difficulty can make a single global quantile more appropriate across heterogeneous inputs, which improves approximate conditional behavior [@lei2018; @chernozhukov2021; @dewolf2025]. Second, research on uncertainty in deep learning has demonstrated that diversified deep ensembles provide practical, shift-aware signals of difficulty that tend to grow on out-of-distribution inputs [@lakshminarayanan2017]. Using these ideas, this study implements a simple strategy for rescaling the classification score by a calibration-learned difficulty proxy, so that the global threshold becomes conservative under drift, without the need for recalibration or labels.

This thesis develops this design for eelgrass segmentation of multi-year drone imagery. The proposed normalizers are deliberately simple to adopt into existing pipelines. This includes: 1. a linear normalization $s'=(1-p_y)/(1+\lambda V)$ where $V$ summarizes ensemble disagreement and $\lambda$ controls degree of conservativeness and 2. a learned normalizer $s'=s/\hat{a}(V)$ where $\hat{a}(V)=\text{E}[s|V]$ is estimated on the calibration split via quantile binning and interpolation. Both approaches retain standard CP workflow and require no labels at test time or retraining.

# Background and Motivation

Effective eelgrass monitoring requires models that both provide accurate predictions, as well as how certain they are for those predictions. This section reviews the uncertainty concepts used in deep learning.

## Uncertainty in Deep Learning

Uncertainty in predictive modeling is commonly broken up into two components: *epistemic* and *aleotoric*. These two components collectively comprise the total uncertainty in a model's predictions. Aleatoric uncertainty represents the noise inherent in observations, which is often impossible to remove, even with the addition of more data (e.g. sensor noise or true label ambiguity). Even a perfect model cannot remove it. Epistemic uncertainty represents the uncertainty of the model itself, which can be reduced with additional or higher-quality data [@kendall2017].

For modern vision models, raw softmax probabilities are often miscalibrated, meaning the outputted confidence does not match empirical accuracy. This can be especially prevalent under shift, where models can be confident yet wrong. Post-hoc calibration, such as temperature scaling, rescales logits via $softmax(z/T)$ and improves in-distribution calibration without changing accuracy [@guo2017]. Under shift, however, calibration is not enough, as the structure of the errors can change. Thus, it requires an uncertainty representation that integrates decision rules that signal the difficulty of the prediction.

## Deep Ensembles

Deep ensembles can address uncertainty by aggregating predictions from models trained independently with different initializations or architectures. For estimating epistemic uncertainty, member disagreements can be calculated by considering the variance in member probabilities or logits. Given members $\{f_k\}_{k=1}^K$ that produce $p^{(k)}(x)$, $V(x) = Var_k(p_y^{(k)}(x))$. Ensembles have not only been found to be more robust under shift, but taking epistemic uncertainty specifically into account can greatly increase a model's predictive uncertainty under dataset shift [@ovadia2019]. Additionally, ensembles require no specialized inference and can easily integrate with existing training code, providing a difficulty proxy for uncertainty. This does come at the additional computational cost of training additional models. However, many models are often trained in pursuit of classification accuracy, which, when ensembled, can outperform individual models.

## Semantic Segmentation in Environmental Monitoring

Semantic image segmentation is a deep learning neural network technique that assigns class labels to each pixel within an image, delineating objects. Recent work demonstrates that deep networks can accurately classify intertidal eelgrass from drone imagery at useful resolutions [@tallam2023]. Environmental monitoring can pose unique challenges, as not only are annotations costly and sometimes ambiguous, but the habitat is constantly shifting over seasonal and annual time scales. Even at the same site, small variations such as time of day, tidal height, etc, can alter the conditions of the imagery. 

## Conformal Prediction Foundations

Conformal prediction provides a model and distribution-free technique to convert probability scores into prediction sets with finite-sample marginal coverage under exchangeability  [@angelopoulos2021]. For a classifier with probabilities $p(x)$, a simple *nonconformity score* is $s=1-p_y$, which is small when the model is confident in the true class. In split conformal prediction, scores are computed on a calibration set $C$ of size $n$ and the finite-sample-corrected quantile is selected $\hat{q}_\alpha = \operatorname{Quantile}_{\lceil (n + 1)(1 - \alpha) \rceil / n}(\{s_i\})$, then the set value prediction is outputed $$C_\alpha(x)=\{y:s\leq\hat{q}_\alpha\}$$
This procedure guarantees $P\{Y∈C_\alpha(X)\}\geq1-\alpha$ for new points exchangeable with $C$, regardless of the underlying model. Two limitations of conformal prediction motivate this thesis. The first limitation is that the guarantee is marginal. This means there is no guarantee of conditional coverage, that the per class coverage will be $\geq$ target, and that coverage is not guaranteed across differing inputs, such as varying difficulty. Second, conformal prediction has mathematical validity due to the concept of symmetry (data could have been seen in any order) as long as the data is exchangeable. Coverage guarantees are lost as distribution drifts, and the model can become overly confident, leading to under-coverage. To address these problems, we need to use an adaptive variant of conformal prediction, which is more conservative for harder inputs.

## Adaptive or Shift-Aware CP

There is a substantial amount of literature that aims to make CP adaptive to heterogeneity or robust during dataset drift. 

In regression, locally-weighted CP normalizes residuals by an estimate of local spread to equalize scores for a global quantile under heterogeneity [@lei2018]. Distributional conformal prediction (DCP) transforms the score using an estimate of the conditional cumulative distribution function. If the distribution of the score is stable conditional on covariates, then a single global quantile should be appropriate across those covariates [@chernozhukov2021]. 

The two ideas loosely connect to this variance score normalization. Let $S=s(X,Y)$ denote the standard noncomformity score and let $V=V(X)$ be a one-dimensional proxy from the deep ensemble (member-probability variance after temperature scaling). We can use this proxy to stabilize the score with respect to $V$, rather than estimating an ultra-high-dimensional $F_{S|X}$ as in DCP. Additionally, we do not estimate a full conditional CDF transform and instead normalize under a multiplicative difficulty assumption $$S=a(V)U,\quad a(V)>0,\quad U \perp \!\!\! \perp V.$$ Then, $S'=S/a(V)\approx U$, so $S'|V$ becomes approximately invariant, and locally tuned to difficulty. Normalization targets the actual failure under shift, that the global threshold will become too lenient for the hard OOD inputs. On the calibration set, we estimate $\hat{a} \approx E[S|V=v]$ via quantile binning and smooth interpolation, and then conformalize the normalized score $S’=S/\hat{a}(V)$. Thus, if the conditional distribution $S|V=v$ is roughly the same across years: $$S∣V=v (calibration)\approx S∣V=v (test),$$ then we expect the normalized scores to be approximately stable conditional on $V$. Then the global quantile will be appropriate across the shifted years. Intuitively, pixels with large member disagreements (hard pixels) would have larger scores. Dividing by $\hat{a}(V)$ will equalize the scores, so that hard and easy pixels are all scored equally, being precise on easy inputs and conservative on difficult ones. By using a difficulty signal (epistemic uncertainty) that is specifically “shift-aware,” we expect that the score becomes shift-aware as well. 

# Methodology

## Data

## Model Training + Pipeline

## Uncertainty Analysis

## Conformalizing

### Split CP for Classification

### Variance-Aware Score Normalization

#### Parametric linear shrink

#### Nonparametric normalization

## CP Evaluation

Set Composition, Spatial Equality, and Conditional Coverage

# Results

## Ensemble Results

## In-Distribution Evaluation

## Temporal OOD (2022)

global coverage and set composition

## Class Conditional Coverage

## Spatial Robustness within 2022

## Sensitivity

# Discussion

# Limitations

# Conclusion

## Computational Details

# REFERENCES {.unnumbered}

::: {#refs}
:::

\appendix

<!-- Store original definitions formatting -->

\let\oldclearpage\clearpage
\let\oldcleardoublepage\cleardoublepage

<!-- Disable page breaks -->

\let\clearpage\relax
\let\cleardoublepage\relax

# Super Cool Fancy Function

<!-- Restore original formatting -->

\let\clearpage\oldclearpage
\let\cleardoublepage\oldcleardoublepage
